syntax = "proto3";

package openbot.dreamview;

import "openbot/common_msgs/chassis_msgs/chassis.proto";
import "openbot/common_msgs/monitor_msgs/monitor_log.proto";
import "openbot/common_msgs/basic_msgs/pnc_point.proto";
import "openbot/common_msgs/perception_msgs/perception_obstacle.proto";
import "openbot/common_msgs/planning_msgs/planning_internal.proto";
import "openbot/common_msgs/prediction_msgs/feature.proto";
import "openbot/common_msgs/routing_msgs/geometry.proto";
import "openbot/common_msgs/config_msgs/vehicle_config.proto";

// Next-id: 4
message PolygonPoint {
  double x = 1;
  double y = 2;
  double z = 3;

  // Gaussian probability information
  openbot.common.GaussianInfo gaussian_info = 4;
}

// Next-id: 3
message Prediction {
  double probability = 1;
  repeated PolygonPoint predicted_trajectory = 2;
}

// Next-id: 13
message Decision {
  enum Type {
    IGNORE = 0;    // Ignore the object
    STOP = 1;      // Stop at a distance from the object
    NUDGE = 2;     // Go around the object
    YIELD = 3;     // Go after the object
    OVERTAKE = 4;  // Go before the object
    FOLLOW = 5;    // Follow the object in the same lane
    SIDEPASS = 6;  // Pass the object in neighboring lane
  }
  Type type = 1;

  // Shape Info
  // Can be used for corners of nudge region
  repeated PolygonPoint polygon_point = 2;

  // Position Info
  // Can be used for stop fence
  double heading = 3;
  double latitude = 4;
  double longitude = 5;
  double position_x = 6;
  double position_y = 7;
  double length = 8;
  double width = 9;
  double height = 10;

  enum StopReasonCode {
    STOP_REASON_HEAD_VEHICLE = 0;
    STOP_REASON_DESTINATION = 1;
    STOP_REASON_PEDESTRIAN = 2;
    STOP_REASON_OBSTACLE = 3;
    STOP_REASON_SIGNAL = 100;
    STOP_REASON_STOP_SIGN = 101;
    STOP_REASON_YIELD_SIGN = 102;
    STOP_REASON_CLEAR_ZONE = 103;
    STOP_REASON_CROSSWALK = 104;
    STOP_REASON_EMERGENCY = 105;
    STOP_REASON_NOT_READY = 106;
    STOP_REASON_PULL_OVER = 107;
  }
  StopReasonCode stopReason = 11;
  openbot.routing.ChangeLaneType change_lane_type = 12;
}

// Next-id: 41
message Object {
  // ID
  string id = 1;  // primary identifier for each object

  // Shape Info
  repeated PolygonPoint polygon_point = 2;  // corners of an object

  // Position Info
  double heading = 3;
  double latitude = 4;
  double longitude = 5;
  double position_x = 6;
  double position_y = 7;
  double length = 8;
  double width = 9;
  double height = 10;

  // Motion Info
  // For objects with motion info such as ADC.
  double speed = 11;               // in m/s, can be negative
  double speed_acceleration = 12;  // in m/s^2
  double speed_jerk = 13;
  double spin = 14;
  double spin_acceleration = 15;
  double spin_jerk = 16;
  double speed_heading = 17;
  double kappa = 18;
  double dkappa = 35;

  // Signal Info
  // For objects with signals set and current signal such as Traffic Light,
  // Stop Sign, and Vehicle Signal.
  repeated string signal_set = 19;
  string current_signal = 20;

  // Time Info
  double timestamp_sec = 21;

  // Decision Info
  repeated Decision decision = 22;
  bool yielded_obstacle = 32;

  // Chassis Info
  // For ADC
  double throttle_percentage = 23;
  double brake_percentage = 24;
  double steering_percentage = 25;
  double steering_angle = 26;
  double steering_ratio = 27;
  int32 battery_percentage = 38;
  openbot.canbus.Chassis.GearPosition gear_location = 39;
  enum DisengageType {
    DISENGAGE_NONE = 0;
    DISENGAGE_UNKNOWN = 1;
    DISENGAGE_MANUAL = 2;
    DISENGAGE_EMERGENCY = 3;
    DISENGAGE_AUTO_STEER_ONLY = 4;
    DISENGAGE_AUTO_SPEED_ONLY = 5;
    DISENGAGE_CHASSIS_ERROR = 6;
  };

  DisengageType disengage_type = 28;

  enum Type {
    UNKNOWN = 0;
    UNKNOWN_MOVABLE = 1;
    UNKNOWN_UNMOVABLE = 2;
    PEDESTRIAN = 3;  // pedestrian, usually determined by moving behavior.
    BICYCLE = 4;     // bike, motor bike.
    VEHICLE = 5;     // passenger car or truck.
    VIRTUAL = 6;     // virtual object created by decision module.
    CIPV = 7;        // closest in-path vehicle determined by perception module.
  };

  Type type = 29;  // obstacle type
  // obstacle sub-type
  openbot.perception.PerceptionObstacle.SubType sub_type = 34;
  repeated Prediction prediction = 30;

  // perception confidence level. Range: [0,1]
  double confidence = 31 ;
  openbot.prediction.ObstaclePriority obstacle_priority = 33;
  openbot.prediction.ObstacleInteractiveTag interactive_tag = 40;

  // v2x for perception obstacle
  openbot.perception.PerceptionObstacle.Source source = 36;  // source type
  // v2x use case info
  openbot.perception.V2XInformation v2x_info = 37;
}

message DelaysInMs {
  double chassis = 1;
  double localization = 3;
  double perception_obstacle = 4;
  double planning = 5;
  double prediction = 7;
  double traffic_light = 8;
  double control = 9;
}

message RoutePath {
  repeated PolygonPoint point = 1;
}

message Latency {
  double timestamp_sec = 1;
  double total_time_ms = 2;
}

message MapElementIds {
  repeated string lane = 1;
  repeated string crosswalk = 2;
  repeated string junction = 3;
  repeated string signal = 4;
  repeated string stop_sign = 5;
  repeated string yield = 6;
  repeated string overlap = 7;
  repeated string road = 8;
  repeated string clear_area = 9;
  repeated string parking_space = 10;
  repeated string speed_bump = 11;
  repeated string pnc_junction = 12;
}

message ControlData {
  double timestamp_sec = 1;
  double station_error = 2;
  double lateral_error = 3;
  double heading_error = 4;
  openbot.common.TrajectoryPoint current_target_point = 5;
}

message Notification {
  double timestamp_sec = 1;
  openbot.common.monitor.MonitorMessageItem item = 2;
}

message SensorMeasurements {
  repeated Object sensor_measurement = 1;
}

// Next-id: 31
message SimulationWorld {
  // Timestamp in milliseconds
  double timestamp = 1;

  // Sequence number
  uint32 sequence_num = 2;

  // Objects in the world and the associated predictions/decisions
  repeated Object object = 3;

  // Autonomous driving cars
  Object auto_driving_car = 4;

  // Planning signal
  Object traffic_signal = 5;

  // Routing paths
  repeated RoutePath route_path = 6;
  // Timestamp of latest routing
  double routing_time = 7;

  // Planned trajectory
  repeated Object planning_trajectory = 8;

  // Main decision
  Object main_stop = 9;
  Object main_decision = 10;

  // Speed limit
  double speed_limit = 11;

  // Module delays
  DelaysInMs delay = 12;

  // Notification
  openbot.common.monitor.MonitorMessage monitor = 13
     ;
  repeated Notification notification = 14;

  // Engage advice from planning
  string engage_advice = 15;

  // Module latency
  map<string, Latency> latency = 16;

  MapElementIds map_element_ids = 17;
  uint64 map_hash = 18;
  double map_radius = 19;

  // Planning data
  openbot.planning_internal.PlanningData planning_data = 20;

  // GPS
  Object gps = 21;

  // Lane Markers from perception
  openbot.perception.LaneMarkers lane_marker = 22;

  // Control data
  ControlData control_data = 23;

  // Relative Map
  repeated openbot.common.Path navigation_path = 24;

  // RSS info
  bool is_rss_safe = 25;

  // Shadow localization
  Object shadow_localization = 26;

  // Perception detected signals
  repeated Object perceived_signal = 27;

  // A map from a story name to whether it is on
  map<string, bool> stories = 28;

  // A map from a sensor_id to a group of sensor_measurements
  map<string, SensorMeasurements> sensor_measurements = 29;

  bool is_siren_on = 30;

  // vehicle param
  openbot.common.VehicleParam vehicle_param = 31;
}
